{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hanna Born,\n",
    "# ANLY 590 HW2.1\n",
    "\n",
    "# References:\n",
    "# code files provided on class github at https://github.com/jh2343/590-CODES\n",
    "# https://towardsdatascience.com/linear-regression-derivation-d362ea3884c2\n",
    "# http://pillowlab.princeton.edu/teaching/mathtools16/slides/lec10_LeastSquaresRegression.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from   scipy.optimize import minimize\n",
    "\n",
    "#USER PARAMETERS\n",
    "IPLOT=True\n",
    "INPUT_FILE='weight.json'\n",
    "FILE_TYPE=\"json\"\n",
    "DATA_KEYS=['x','is_adult','y']\n",
    "X_KEYS = ['x']\n",
    "Y_KEYS = ['y'] #'is_adult'\n",
    "OPT_ALGO='BFGS'\n",
    "\n",
    "PARADIGM='batch'\n",
    "# ---- MODEL SELECTION ---------------------------------------------------------\n",
    "# choosing second logistic model for columns corresponding to age vs. weight relationship\n",
    "# choose model type by uncommenting\n",
    "# model_type=\"linear\";   NFIT=2; xcol=1; ycol=2;       # age vs. weight (linear)\n",
    "model_type=\"logistic\"; NFIT=4; xcol=1; ycol=2;       # age vs weight (logistic)\n",
    "# model_type=\"logistic\";   NFIT=4; xcol=2; ycol=0;     # weight vs. adult/child\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "#READ FILE\n",
    "with open(INPUT_FILE) as f:\n",
    "\tinput1 = json.load(f)  #read into dictionary\n",
    "\n",
    "#CONVERT DICTIONARY INPUT AND OUTPUT MATRICES #SIMILAR TO PANDAS DF   \n",
    "X=[]; Y=[]\n",
    "for key in input1.keys():\n",
    "\tif(key in X_KEYS): X.append(input1[key])\n",
    "\tif(key in Y_KEYS): Y.append(input1[key])\n",
    "\n",
    "#MAKE ROWS=SAMPLE DIMENSION (TRANSPOSE)\n",
    "X=np.transpose(np.array(X))\n",
    "Y=np.transpose(np.array(Y))\n",
    "print('--------INPUT INFO-----------')\n",
    "print(\"X shape:\",X.shape); print(\"Y shape:\",Y.shape,'\\n')\n",
    "\n",
    "#TAKE MEAN AND STD DOWN COLUMNS (I.E DOWN SAMPLE DIMENSION)\n",
    "XMEAN=np.mean(X,axis=0); XSTD=np.std(X,axis=0) \n",
    "YMEAN=np.mean(Y,axis=0); YSTD=np.std(Y,axis=0) \n",
    "\n",
    "#NORMALIZE \n",
    "X=(X-XMEAN)/XSTD;  Y=(Y-YMEAN)/YSTD  \n",
    "\n",
    "#------------------------\n",
    "#PARTITION DATA\n",
    "#------------------------\n",
    "#TRAINING: \t DATA THE OPTIMIZER \"SEES\"\n",
    "#VALIDATION: NOT TRAINED ON BUT MONITORED DURING TRAINING\n",
    "#TEST:\t\t NOT MONITORED DURING TRAINING (ONLY USED AT VERY END)\n",
    "\n",
    "f_train=0.8; f_val=0.15; f_test=0.05;\n",
    "\n",
    "if(f_train+f_val+f_test != 1.0):\n",
    "\traise ValueError(\"f_train+f_val+f_test MUST EQUAL 1\");\n",
    "\n",
    "#PARTITION DATA\n",
    "rand_indices = np.random.permutation(X.shape[0])\n",
    "CUT1=int(f_train*X.shape[0]); \n",
    "CUT2=int((f_train+f_val)*X.shape[0]); \n",
    "train_idx, val_idx, test_idx = rand_indices[:CUT1], rand_indices[CUT1:CUT2], rand_indices[CUT2:]\n",
    "print('------PARTITION INFO---------')\n",
    "print(\"train_idx shape:\",train_idx.shape)\n",
    "print(\"val_idx shape:\"  ,val_idx.shape)\n",
    "print(\"test_idx shape:\" ,test_idx.shape)\n",
    "\n",
    "#------------------------\n",
    "#MODEL\n",
    "#------------------------\n",
    "def model(x,p):\n",
    "\tif(model_type==\"linear\"):   return  p[0]*x+p[1]  \n",
    "\tif(model_type==\"logistic\"): return  p[0]+p[1]*(1.0/(1.0+np.exp(-(x-p[2])/(p[3]+0.0001))))\n",
    "\n",
    "#FUNCTION TO MAKE VARIOUS PREDICTIONS FOR GIVEN PARAMETERIZATION\n",
    "def predict(p):\n",
    "\tglobal YPRED_T,YPRED_V,YPRED_TEST,MSE_T,MSE_V\n",
    "\tYPRED_T=model(X[train_idx],p)\n",
    "\tYPRED_V=model(X[val_idx],p)\n",
    "\tYPRED_TEST=model(X[test_idx],p)\n",
    "\tMSE_T=np.mean((YPRED_T-Y[train_idx])**2.0)\n",
    "\tMSE_V=np.mean((YPRED_V-Y[val_idx])**2.0)\n",
    "\n",
    "#------------------------\n",
    "#LOSS FUNCTION\n",
    "#------------------------\n",
    "def loss(p,index_2_use):\n",
    "\terrors=model(X[index_2_use],p)-Y[index_2_use]  #VECTOR OF ERRORS\n",
    "\ttraining_loss=np.mean(errors**2.0)\t\t\t\t#MSE\n",
    "\treturn training_loss\n",
    "\n",
    "\n",
    "#SAVE HISTORY FOR PLOTTING AT THE END\n",
    "iteration=0; iterations=[]; loss_train=[];  loss_val=[]\n",
    "#------------------------\n",
    "#OPTIMIZER FUNCTION\n",
    "#------------------------\n",
    "def optimizer(f,xi, algo='GD', LR=0.01):\n",
    "\tglobal epoch,epochs, loss_train,loss_val \n",
    "\t# x0=initial guess, (required to set NDIM)\n",
    "\t# algo=GD or MOM\n",
    "\t# LR=learning rate for gradient decent\n",
    "\n",
    "\t#PARAM\n",
    "\titeration=1\t\t\t#ITERATION COUNTER\n",
    "\tdx=0.0001\t\t\t#STEP SIZE FOR FINITE DIFFERENCE\n",
    "\tmax_iter=5000\t\t#MAX NUMBER OF ITERATION\n",
    "\ttol=10**-10\t\t\t#EXIT AFTER CHANGE IN F IS LESS THAN THIS \n",
    "\tNDIM=len(xi)\t\t#DIMENSION OF OPTIIZATION PROBLEM\n",
    "\n",
    "\t#OPTIMIZATION LOOP\n",
    "\twhile(iteration<=max_iter):\n",
    "\n",
    "\t\t#-------------------------\n",
    "\t\t#DATASET PARITION BASED ON TRAINING PARADIGM\n",
    "\t\t#-------------------------\n",
    "\t\tif(PARADIGM=='batch'):\n",
    "\t\t\tif(iteration==1): index_2_use=train_idx\n",
    "\t\t\tif(iteration>1):  epoch+=1\n",
    "\t\telse:\n",
    "\t\t\tprint(\"REQUESTED PARADIGM NOT CODED\"); exit()\n",
    "\n",
    "\t\t#-------------------------\n",
    "\t\t#NUMERICALLY COMPUTE GRADIENT \n",
    "\t\t#-------------------------\n",
    "\t\tdf_dx=np.zeros(NDIM);\t#INITIALIZE GRADIENT VECTOR\n",
    "\t\tfor i in range(0,NDIM):\t#LOOP OVER DIMENSIONS\n",
    "\n",
    "\t\t\tdX=np.zeros(NDIM);  #INITIALIZE STEP ARRAY\n",
    "\t\t\tdX[i]=dx; \t\t\t#TAKE SET ALONG ith DIMENSION\n",
    "\t\t\txm1=xi-dX; \t\t\t#STEP BACK\n",
    "\t\t\txp1=xi+dX; \t\t\t#STEP FORWARD \n",
    "\n",
    "\t\t\t#CENTRAL FINITE DIFF\n",
    "\t\t\tgrad_i=(f(xp1,index_2_use)-f(xm1,index_2_use))/dx/2\n",
    "\n",
    "\t\t\t# UPDATE GRADIENT VECTOR \n",
    "\t\t\tdf_dx[i]=grad_i \n",
    "\t\t\t\n",
    "\t\t#TAKE A OPTIMIZER STEP\n",
    "\t\tif(algo==\"GD\"):  xip1=xi-LR*df_dx \n",
    "\t\tif(algo==\"MOM\"): print(\"REQUESTED ALGORITHM NOT CODED\"); exit()\n",
    "\n",
    "\n",
    "\t\tif(iteration==0):\n",
    "\t\t\tprint(\"iteration \\t epoch \\t MSE_T \\t MSE_V\")\n",
    "\t\tif(iteration%250==0):\n",
    "\t\t\tprint(iteration,\"\t\",epoch,\"\t\",MSE_T,\"\t\",MSE_V)\n",
    "\n",
    "\t\t#REPORT AND SAVE DATA FOR PLOTTING\n",
    "\t\tif(iteration%1==0):\n",
    "\t\t\tpredict(xi)\t#MAKE PREDICTION FOR CURRENT PARAMETERIZATION\n",
    "\n",
    "\t\t\t#UPDATE\n",
    "\t\t\tepochs.append(epoch); \n",
    "\t\t\tloss_train.append(MSE_T);  loss_val.append(MSE_V);\n",
    "\n",
    "\t\t\t#STOPPING CRITERION (df=change in objective function)\n",
    "\t\t\tdf=np.absolute(f(xip1,index_2_use)-f(xi,index_2_use))\n",
    "\t\t\tif(df<tol):\n",
    "\t\t\t\tprint(\"STOPPING CRITERION MET (STOPPING TRAINING)\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\txi=xip1 #UPDATE FOR NEXT PASS\n",
    "\t\titerations.append(iteration)\n",
    "\t\titeration=iteration+1\n",
    "\n",
    "\treturn xi\n",
    "\n",
    "\n",
    "#------------------------\n",
    "#FIT MODEL\n",
    "#------------------------\n",
    "\n",
    "#RANDOM INITIAL GUESS FOR FITTING PARAMETERS\n",
    "po=np.random.uniform(2,1.,size=NFIT)\n",
    "\n",
    "#TRAIN MODEL USING OPTIMIZER(MINIMIZER)\n",
    "# have the optimizer take the loss function as an argument as well as various default options\n",
    "popt=optimizer(loss,po)\n",
    "print(\"OPTIMAL PARAM:\",p_final)\n",
    "predict(popt)\n",
    "    \n",
    "\n",
    "\n",
    "#UN-NORMALIZE\n",
    "def unnorm_x(x): \n",
    "\treturn XSTD*x+XMEAN  \n",
    "def unnorm_y(y): \n",
    "\treturn YSTD*y+YMEAN \n",
    "\n",
    "# generate plots\n",
    "#FUNCTION PLOTS\n",
    "if(IPLOT):\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(unnorm_x(X[train_idx]), unnorm_y(Y[train_idx]), 'o', label='Training set')\n",
    "\tax.plot(unnorm_x(X[test_idx]), unnorm_y(Y[test_idx]), 'x', label='Test set')\n",
    "\tax.plot(unnorm_x(X[val_idx]), unnorm_y(Y[val_idx]), '*', label='Validation set')\n",
    "\tax.plot(unnorm_x(x[train_idx]),unnorm_y(YPRED_T), '.', c='red', label='Model')\n",
    "\tplt.xlabel('x', fontsize=18)\n",
    "\tplt.ylabel('y', fontsize=18)\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "#PARITY PLOTS\n",
    "if(IPLOT):\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(model(X[train_idx],popt), Y[train_idx], 'o', label='Training set')\n",
    "\tax.plot(model(X[val_idx],popt), Y[val_idx], 'o', label='Validation set')\n",
    "\tplt.xlabel('y predicted', fontsize=18)\n",
    "\tplt.ylabel('y data', fontsize=18)\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "#MONITOR TRAINING AND VALIDATION LOSS  \n",
    "if(IPLOT):\n",
    "\tfig, ax = plt.subplots()\n",
    "\tax.plot(iterations, loss_train, 'o', label='Training loss')\n",
    "\tax.plot(iterations, loss_val, 'o', label='Validation loss')\n",
    "\tplt.xlabel('optimizer iterations', fontsize=18)\n",
    "\tplt.ylabel('loss', fontsize=18)\n",
    "\tplt.legend()\n",
    "\tplt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
